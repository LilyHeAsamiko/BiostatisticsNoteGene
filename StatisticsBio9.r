library(stats)
library(dplyr)
set.seed(1)
m = 10000
n = 24
x = matrix(rnorm(m*n),m,n)
colnames(x)=1:n
hc1 = hclust(dist(x),method='ward')
plot(hc1)

library('pvclust')
library('dendextend')
m = 10000
n = 24
x = matrix(rnorm(m*n),m,n)
colnames(x)=1:n
hc = pvclust(x,method.hclust='ward')
par(mfrow=c(1,2))
plot(hc)
ct = cutree(as.dendrogram(hc1),h=143)
hist(ct)
#cutree(as.dendrogram(hc),h=143)
#monte carlo
sd(rnorm(length(ct),mean(ct),sd(ct)))
sd(runif(length(ct),min(ct),max(ct)))

#Run kmeans with 4 centers for the blood RNA data:
  
library(GSE5859Subset)
data(GSE5859Subset)
plot(as.dendrogram(hc1 = hclust(dist(t(geneExpression)))))
month = format( sampleInfo$date, "%m")
mon = factor( month)
year = format( sampleInfo$date, "%y")
yea = factor( year)
#Set the seed to 10, set.seed(10) right before running kmeans with 5 centers.
set.seed(10)
#Explore the relationship of clusters and information in sampleInfo. Which of the following best describes what you find?
ct = cutree(hc1,k=5)
#plot(as.dendrogram(ct))
table(ct,sampleInfo$group)
#ct  0 1
#  1 8 3
#  2 0 5
#  3 2 3
#  4 1 1
#  5 1 0
table(sampleInfo$group,mon)
#mon
#  06 10
#0  9  3
#1  3  9
table(sampleInfo$group,sampleInfo$ethnicity)
#  ASN CEU HAN
#0  11   1   0
#1  12   0   0
#D

#A) sampleInfo$group is driving the clusters as the 0s and 1s are in completely different clusters.
#B) The year is driving the clusters.
#C) Date is driving the clusters.
#D) The clusters don’t depend on any of the column of sampleInfo

#Load the data:
  
library(GSE5859Subset)
data(GSE5859Subset)
#Pick the 25 genes with the highest across sample variance. This function might help:
  
install.packages("matrixStats")
library(matrixStats)
?rowMads ##we use mads due to a outlier sample
#Use heatmap.2 to make a heatmap showing the sampleInfo$group with color, the date as labels, the rows labelled with chromosome, and scaling the rows.
rmds = rowMads(matrixStats)
plot(rmds)
hist(rmds)
#indx = filter(rank(rmds),length(mds)-25+1:length(mds)) %>% unlist
#indx = rank(rmds)[which(rank(rmds) in length(rmds)-25+1:length(rmds))]
geneAnnotation['Rrmds'] = rank(rmds)#rank with increasing order
gene25 = filter(geneAnnotation, Rrmds %in% c(length(rmds)-25+1:length(rmds)))
addData = data.frame(geneExpression[gene25$PROBEID,])
addData['PROBEID'] = gene25$PROBEID
info25 = cbind(sampleInfo,t(data.frame(geneExpression[gene25$PROBEID,])))
geneExpression25 = merge(gene25,addData,by = 'PROBEID')

geneExpression25['DATE'] = sampleInfo$date
#xlab strCol = ,ylab strRow
if (!require("gplots")) install.packages("gplots")
library(gplots)
#data <- read.table(".txt",header=TRUE,row.names = 1)
#data <- as.matrix(geneExpression25[,6:29])
data <- info25[,4:29] #24*26
#data.rownames = info25$date
#data.colnames = c('group',geneExpression25$CHR)
rownames(data) = paste0(info25$filename,info25$date)
colnames(data) = c('group',geneExpression25$CHR)

coul <- colorRampPalette(colors = c("#4A73EE","white","#F46161"))(100)

heatmap.2(as.matrix(data),scale = "row",col = coul,
          main = 'hotplot',xlab = 'Chromosomes', ylab = 'Date',
          margins = c(10,20),  #行名和列名的边距。用于调整热图大小
          ColSideColors = rainbow(ncol(data)),  #注释列的水平边栏的颜色向量（行也类似）。
          
          Rowv = FALSE,   #determines if and how the row dendrogram should be reordered.即是否对行聚类并重新排序。取值为TRUE、FALSE或整数向量。
          Colv = FALSE,   #determines if and how the column dendrogram should be reordered. 是否对列进行聚类并重新排序。取值为TRUE、FALSE或“Rowv”（表示与行的处理方式相同）
          dendrogram="none", #whether to draw 'none', 'row', 'column' or 'both' dendrograms.即是否画出聚类树
          
          colCol = c(rep('red',ncol(data))), #列标签的颜色向量（行也类似）
          
#          rowsep = c(2,4),colsep = 4, #在指定的行/列插入gap
#          sepcolor = "white",  #gap的颜色
#          sepwidth=c(0.1,0.1),  #行、列中gap的大小
          
          trace = 'column',  # 是否在c("column","row","both","none")绘制实线,线越靠近单元格四侧，值越大或小。
          tracecol="green",  #实线颜色
          linecol='blue'   #中间虚线颜色
)

#What do we learn from this heatmap?
  
#A) The data appears as if it was generated by rnorm.
#B) Some genes in chr1 are very variable.
#C) A group of chrY genes are higher in group 0 and appear to drive the clustering. Within those clusters there appears to be clustering by month.
#D) A group of chrY genes are higher in October compared to June and appear to drive the clustering. Within those clusters there appears to be clustering by samplInfo$group.

#Create a large data set of random data that is completely independent of sampleInfo$group like this:
library('genefilter')

set.seed(17)
m = nrow(geneExpression)
n = ncol(geneExpression)
x = matrix(rnorm(m*n),m,n)
g = factor(sampleInfo$g )
#Create two heatmaps with these data. Show the group g either with labels or colors. First, take the 50 genes with smallest p-values obtained with rowttests. Then, take the 50 genes with largest standard deviations.
rtts = rowttests(geneExpression,g)
Rrtts = rank(rtts$p.value)
Rrttssd = rank(rtts$statistic)
info50 = cbind(rtts,Rrtts,Rrttssd)
#gene501 = geneExpression[filter(info50,Rrtts %in% c(1:50)),]
#gene502 = geneExpression[filter(info50,Rrttssd %in% c(length(Rrttssd)-49:length(Rrttssd))),]
gene501 = geneExpression[rownames(filter(info50,Rrtts %in% c(1:50))),]
gene502 = geneExpression[rownames(filter(info50,Rrttssd %in% c((length(info50$Rrttssd)-49):length(info50$Rrttssd)))),]
#par(mfrow = c(1,2))
coul <- colorRampPalette(colors = c("#4A73EE","white","#F46161"))(100)

heatmap.2(as.matrix(gene501),scale = "row",col = coul,
          main = 'hotplot',xlab = 'Person', ylab = 'Probeid',
          margins = c(10,20),  #行名和列名的边距。用于调整热图大小
          ColSideColors = rainbow(ncol(gene501)),  #注释列的水平边栏的颜色向量（行也类似）。
          
          Rowv = FALSE,   #determines if and how the row dendrogram should be reordered.即是否对行聚类并重新排序。取值为TRUE、FALSE或整数向量。
          Colv = FALSE,   #determines if and how the column dendrogram should be reordered. 是否对列进行聚类并重新排序。取值为TRUE、FALSE或“Rowv”（表示与行的处理方式相同）
          dendrogram="none", #whether to draw 'none', 'row', 'column' or 'both' dendrograms.即是否画出聚类树
          
          colCol = c(rep('red',ncol(gene501))), #列标签的颜色向量（行也类似）
          
          #          rowsep = c(2,4),colsep = 4, #在指定的行/列插入gap
          #          sepcolor = "white",  #gap的颜色
          #          sepwidth=c(0.1,0.1),  #行、列中gap的大小
          
          trace = 'column',  # 是否在c("column","row","both","none")绘制实线,线越靠近单元格四侧，值越大或小。
          tracecol="green",  #实线颜色
          linecol='blue'   #中间虚线颜色
)

coul <- colorRampPalette(colors = c("#4A73EE","white","#F46161"))(100)

heatmap.2(as.matrix(gene502),scale = "row",col = coul,
          main = 'hotplot',xlab = 'Person', ylab = 'Probeid',
          margins = c(10,20),  #行名和列名的边距。用于调整热图大小
          ColSideColors = rainbow(ncol(gene502)),  #注释列的水平边栏的颜色向量（行也类似）。
          
          Rowv = FALSE,   #determines if and how the row dendrogram should be reordered.即是否对行聚类并重新排序。取值为TRUE、FALSE或整数向量。
          Colv = FALSE,   #determines if and how the column dendrogram should be reordered. 是否对列进行聚类并重新排序。取值为TRUE、FALSE或“Rowv”（表示与行的处理方式相同）
          dendrogram="none", #whether to draw 'none', 'row', 'column' or 'both' dendrograms.即是否画出聚类树
          
          colCol = c(rep('red',ncol(gene502))), #列标签的颜色向量（行也类似）
          
          #          rowsep = c(2,4),colsep = 4, #在指定的行/列插入gap
          #          sepcolor = "white",  #gap的颜色
          #          sepwidth=c(0.1,0.1),  #行、列中gap的大小
          
          trace = 'column',  # 是否在c("column","row","both","none")绘制实线,线越靠近单元格四侧，值越大或小。
          tracecol="green",  #实线颜色
          linecol='blue'   #中间虚线颜色
)


#Which of the following statements is true?
  
#A) There is no relationship between g and x, but with 8,793 tests some will appear significant by chance. Selecting genes with the t-test gives us a deceiving result.
#B) These two techniques produced similar heatmaps.
#C) Selecting genes with the t-test is a better technique since it permits us to detect the two groups. It appears to find hidden signals.
#D) The genes with the largest standard deviation add variability to the plot and do not let us find the differences between the two groups.

#C

#Conditional Expectations Exercises
#Exercises
#Throughout these exercises it will be useful to remember that when our data are 0s and 1s, probabilities and expectations are the same thing. We can do the math, but here is some R code:
n = 1000
y = rbinom(n,1,0.25)
##proportion of ones Pr(Y)
sum(y==1)/length(y)
##expectaion of Y
mean(y)
#Generate some random data to imitate heights for men (0) and women (1):
n = 10000
set.seed(1)
men = rnorm(n,176,7) #height in centimeters
women = rnorm(n,162,7) #height in centimeters
y = c(rep(0,n),rep(1,n))#men woman labels
x = round(c(men,women))
##mix it up
ind = sample(seq(along=y))
y = y[ind]
x = x[ind]
#Using the data generated above, what is the $$E(Y	X=176)$?
fit = lm(y~x)
lb = fit$coefficient[1]+fit$coefficient[2]*176
#0.2494172
round(lb)
#0 
#Now make a plot of $$E(Y	X=x)$$ for x=seq(160,178) using the data generated in exercise 1.
#If you are predicting female or male based on height and want your probability of success to be larger than 0.5, what is the largest height where you predict female ?
par(mfrow=c(1,2))
plot(x,y,xlab='mixedHeight',ylab='mixedLabel',main=paste0('correlation=',signif(cor(x,y),2)))
abline(v=c(160,178),col ='red')
abline(fit,col = 'green')

x0=seq(160,178) 
hist(round(y[x==x0]),xlab='Label of mixedHeights',main = '',xlim =range(y))
abline(v = fit$coefficient[1]+fit$coefficient[2]*176)

filter(xx, )
xx = rank(x)
fid = rep(0,20000) #E(X=?|Y=1)
xn = 1
fidx = sum(y[which(x==min(x))])/length(y[which(x==min(x))])
#indx = which(xx)
#xtmp = xx[a]
#fid[a] = (round(y[xtmp])==1)-sum(y[which(x %in% xtmp)]==1))/sum(y[which(x %in% xtmp)]==1)
#fid[1] = 
for (xtmp in c((min(x)+1):max(x))){
  if (length(y[which(x==xtmp)])){
    fidx = c(fidx,sum(y[which(x==xtmp)])/length(y[which(x==xtmp)]))
    xn = c(xn,length(y[which(x==xtmp)]))}
  else{
    fidx = c(fidx,0)
    xn = c(xn,0)}
}
#
plot(xn,fidx) 
plot(fidx)
which(fidx>0.5)
#[1]  1  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28
#[25] 29 30 31 32 33 34 35 36 37
xn[which(fidx>0.5)]
# [1]   1   1   3   1   3   4   5  19  19  22  34  57  60 114 116 164 209 258
#[19] 319 377 393 498 532 535 610 647 644 675 674 700 664 700 686
min(x)+max(which(fidx>0.5))-1
#168

#Smoothing exercises
#Exercises
#Generate the following data:
n = 10000
set.seed(1)
men = rnorm(n,176,7) #height in centimeters
women = rnorm(n,162,7) #height in centimeters
y = c(rep(0,n),rep(1,n))
x = round(c(men,women))
##mix it up
ind = sample(seq(along=y))
y = y[ind]
x = x[ind]
#Set the seed at 5, set.seed(5) and take a random sample of 250 from:
set.seed(5)
N = 250
ind = sample(length(y),N)
Y = y[ind]
X = x[ind]
#Use loess to estimate $$f(x)=E(Y	X=x)usingthedefaultparameters.Whatisthepredicted
#f(168)$?
fit <- loess(Y~X, degree=1, span=1/3)

newx <- seq(167.5,168.4,len=10) 
smooth <- predict(fit,newdata=data.frame(X=newx))
#1         2         3         4         5         6         7 
#0.6235645 0.6137356 0.6038124 0.5937944 0.5836811 0.5734721 0.5634540 
#8         9        10 
#0.5538947 0.5447651 0.5360362 

par ()
plot(X,Y,col="darkgrey",pch=16)
lines(newx,smooth,col="black",lwd=3)
mean(smooth)
#[1] 0.579021
round(smooth)
#1  2  3  4  5  6  7  8  9 10 
#1  1  1  1  1  1  1  1  1  1 

#The loess estimate above is a random variable. We can compute standard errors for it. Here we use Monte Carlo to demonstrate that it is a random variable. Use Monte Carlo simulation to estimate the standard error of your estimate of f(168).
#Set the seed to 5, set.seed(5) and perform 10000 simulations and report the SE of the loess based estimate.

#'''smooth<-function(h){
#  #f(168)$?
#  n = 10000
#  set.seed(1)
#  men = rnorm(n,176,7) #height in centimeters
#  women = rnorm(n,162,7) #height in centimeters
#  y = c(rep(0,n),rep(1,n))
#  x = round(c(men,women))
#  ##mix it up
#  ind = sample(seq(along=y))
#  y = y[ind]
#  x = x[ind]
#  
#  set.seed(5)
#  m = 250
#  ind = sample(length(y),m)
#  Y = y[ind]
#  X = x[ind]
#  fit <- loess(Y~X, degree=1, span=1/3)
#  newx <- seq(h-0.5,h+0.4,len=10) 
#  mean(predict(fit,newdata=data.frame(X=newx)))
#}
#set.seed(5)
#N = 10000
#fits<-rep(sapply(168,smooth),N)
#SE<-sd(fits)'''

set.seed(5)
N = 10000
smooth0<-function (x,y,h){
  m = 250
  ind = sample(length(y),m)
  Y = y[ind]
  X = x[ind]
  fit <- loess(Y~X, degree=1, span=1/3)
  newx <- seq(h-0.5,h+0.4,len=10) 
  mean(predict(fit,newdata=data.frame(X=newx)))-sum(y[which(x == h)])/length(y[which(x == 168)])
}
#resfits<-rep(smooth0(x,y,168),N)

smooth<-function (x,y,h){
  m = 250
  ind = sample(length(y),m)
  Y = y[ind]
  X = x[ind]
  fit <- loess(Y~X, degree=1, span=1/3)
  newx <- seq(h-0.5,h+0.4,len=10) 
  mean(predict(fit,newdata=data.frame(X=newx)))
}
set.seed(5)
fits = rep(0,N)
ress = rep(0,N)
for (i in 1:N){
#  set.seed(1)
  men = rnorm(n,176,7) #height in centimeters
  women = rnorm(n,162,7) #height in centimeters
  y = c(rep(0,n),rep(1,n))
  x = round(c(men,women))
  ##mix it up
  ind = sample(seq(along=y))
  y = y[ind]
  x = x[ind]
  fits[i] = smooth(x,y,168)
  ress[i] = sum(y[which(x == 168)])/length(y[which(x == 168)])-fits[i]
}
sum(ress^2)/N
#[1] 0.004264523
plot(ress,main = paste0('SE = ',sum(ress^2)/N))

#Cross-validation Exercises
#Exercises
#Load the following dataset:
if(!require(caret)) install.packages('caret')
library(caret)  
if(!require(class)) install.packages('class')
library(class)  
library(GSE5859Subset)
data(GSE5859Subset)
#And define the outcome and predictors. To make the problem more difficult, we will only consider autosomal genes:
  
y = factor(sampleInfo$group)
X = t(geneExpression)
out = which(geneAnnotation$CHR%in%c("chrX","chrY"))
X = X[,-out]
#Use the createFold function in the caret package, set the seed to 1 set.seed(1) and create 10 folds.
set.seed(1)
indx<-createFolds(y, k=10)
#Question: What is the 2nd entry in the fold 3?
#indx$Fold03
#[1] 11 17
#y[indx[[2]]]
#We are going to use kNN. We are going to consider a smaller set of predictors by using filtering genes using t-tests. Specifically, we will perform a t-test and select the m
#genes with the smallest p-values.

#Let m=8
#and k=5
#and train kNN by leaving out the second fold idx[[2]]. How many mistakes do we make on the test set? Remember it is indispensable that you perform the t-test on the training data.
library('genefilter')
rts<-rowttests(geneEXpression)
data = geneExpression[which(order(rts$p.value) <9),]
pred<-knn3Train(train=t(data[,-indx[[2]]]),test = t(data[,indx[[2]]]),cl=y[-indx[[2]]],k=5)
table(true = y[indx[[2]]],pred)
#    pred
#true 0 1
#   0 1 0
#   1 0 1

#Now run through all 5 folds. What is our error rate?
pred1<-knn3Train(train=t(data[,-c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])]),test = t(data[,c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])]),cl=y[-c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])],k=5)
mean(y[c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])]!=pred1)
#[1] 0.2727273
#Now we are going to select the best values of k and m. Use the expand grid function to try out the following values:
  
ms=2^c(1:11)
#ks=seq(1,9,2)
ks = 1:5
indxs = c(list(indx[[1]]),list(c(indx[[1]],indx[[2]],indx[[3]])),list(c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])),list(c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]],indx[[6]],indx[[7]])),list(c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]],indx[[6]],indx[[7]],indx[[8]],indx[[9]])))
params = expand.grid(k=ks,m=ms)
#Now use apply or a for-loop to obtain error rates for each of these pairs of parameters. Which pair of parameters minimizes the error rate?
set.seed(1)
res <- sapply(params, function(params) {
  ##try out each version of k from 1 to 5
  res.k <- sapply(seq_along(params), function(i) {
    ##loop over each of the 10 cross-validation folds
    ##predict the held-out samples using k nearest neighbors
    predtmp <- knn(train=t(geneExpression[params$m,-indxs[params$k]]),
                test=t(geneExpresion[params$m,indxs[params$k]]),
                cl=y[-indxs[params$k]], k = length(y[-params$k]))
    ##the ratio of misclassified samples
    mean(y[indxs[params$k]] != predtmp)
  })
  ##average over the folds
  mean(res.k)
})
resks = rep(0,length(params))
resk<-function(m,k) {
    ##loop over each of the 10 cross-validation folds
    ##predict the held-out samples using k nearest neighbors
    predtmp <- knn3Train(train=t(geneExpression[1:m,-indxs[[k]]]),
                   test=t(geneExpression[1:m,indxs[[k]]]),
                   cl=y[-indxs[[k]]], k = length(y[indxs[[k]]]))
    ##the ratio of misclassified samples
    mean(y[indxs[[k]]] != predtmp)
}
#resks<-sapply(seq_along(params),resk)
for(i in 1:dim(params)[1]){
  mi = params$m[i]
  ki = params$k[i]
  resks[i] = resk(mi,ki)
}
par(mfrow = c(1,2))
plot(params$m,resks,type = 'o',ylab = 'misclassifiation error')
plot(resks,type = 'o',ylab = 'misclassifiation error')
which(order(resks) == 1)
#[1] 35
params$k[which(order(resks) == 1)]
#[1] 5
params$m[which(order(resks) == 1)]
#[1] 128
indxs[[params$k[which(order(resks) == 1)]]]
#[1]  6 14 10 15 11 17 12 16 23  4 21  5  9 18  2 13  8 20  3 22 24
k = length(indxs[[params$k[which(order(resks) == 1)]]])
k
#[1] 21
#Repeat exercise 4, but now perform the t-test filtering before the cross validation. Note how this biases the entire result and gives us much lower estimated error rates.
rts<-rowttests(geneEXpression)
pred<-knn3Train(train=t(data[,-indx[[2]]]),test = t(data[,indx[[2]]]),cl=y[-indx[[2]]],k=5)
table(true = y[indx[[2]]],pred)
reskprefm<-function(m,k) {
  data = geneExpression[which(order(rts$p.value) <=m),]
  ##predict the held-out samples using k nearest neighbors
  predtmp <- knn3Train(train=t(data[,-indxs[[k]]]),
                       test=t(data[,indxs[[k]]]),
                       cl=y[-indxs[[k]]], k = length(y[indxs[[k]]]))
  ##the ratio of misclassified samples
  mean(y[indxs[[k]]] != predtmp)
}
reskprefms = rep(0,length(params))
for(i in 1:dim(params)[1]){
  mi = params$m[i]
  ki = params$k[i]
  reskprefms[i] = reskprefm(mi,ki)
}
par(mfrow = c(1,2))
plot(params$m,reskprefms,type = 'o',ylab = 'misclassifiation error')
plot(reskprefms,type = 'o',ylab = 'misclassifiation error')
which(order(reskprefms) == 1)
#[1] 35
params$k[which(order(reskprefms) == 1)]
#[1] 5
params$m[which(order(reskprefms) == 1)]
#[1] 128
indxs[[params$k[which(order(reskprefms) == 1)]]]
#[1]  6 14 10 15 11 17 12 16 23  4 21  5  9 18  2 13  8 20  3 22 24
k = length(indxs[[params$k[which(order(reskprefms) == 1)]]])
k
#[1] 21
par(mfrow = c(2,2))
plot(params$m,reskprefms,col = 1,ylab = 'm-prefiltered misclassifiation error')
plot(params$m,resks,col =2,ylab = 'misclassifiation error')
plot(reskprefms,col = 1,ylab = 'm-prefilltered misclassifiation error')
plot(resks,col =2,ylab = 'misclassifiation error')

#mean(resks)
#[1] 0.3197757
#mean(reskprefms)
#[1] 0.3258707

#Repeat exercise 3, but now, instead of sampleInfo$group , use
y1 = factor(as.numeric(format( sampleInfo$date, "%m")=="06"))
#What is the minimum error rate now?
pred<-knn3Train(train=t(data[,-indx[[2]]]),test = t(data[,indx[[2]]]),cl=y1[-indx[[2]]],k=5)
table(true = y1[indx[[2]]],pred)
#    pred
#true 0
#   0 1
#   1 1

#Now run through all 5 folds. What is our error rate?
pred11<-knn3Train(train=t(data[,-c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])]),test = t(data[,c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])]),cl=y[-c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])],k=5)
mean(y1[c(indx[[1]],indx[[2]],indx[[3]],indx[[4]],indx[[5]])]!=pred11)
#[1] 0.8181818
#We achieve much lower error rates when predicting date than when predicting the group. Because group is confounded with date, it is very possible that these predictors have no information about group and that our lower 0.5 error rates are due to the confounding with date. We will learn more about this in the batch effect chapter.

